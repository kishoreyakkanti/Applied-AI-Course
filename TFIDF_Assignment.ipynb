{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate idf value\n",
    "\n",
    "def idf(word,dataset):\n",
    "    N=len(dataset) #length of corpus\n",
    "    No_of_docs=0\n",
    "    for i in range(N): \n",
    "        if word in dataset[i].split():  #splitting document by document in corpus\n",
    "            No_of_docs+=1     #word present in no of documents count\n",
    "    if No_of_docs>0:\n",
    "        idfvalue=1+math.log((1+N)/(1+No_of_docs))  #idf formula\n",
    "        return idfvalue\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit(dataset):    \n",
    "    unique_words = set()\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for row in dataset:  # for each document in the dataset\n",
    "            for word in row.split(\" \"):  # for each word in the document\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words)) #unique words in corpus\n",
    "        arr=[]\n",
    "        for word in unique_words:\n",
    "            arr.append(idf(word,corpus))   #calculating idf values for each unique word\n",
    "        list_idf=np.array(arr)\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        return vocab,list_idf\n",
    "    else:\n",
    "        print(\"you need to pass list of sentance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset,vocab):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for idx, row in enumerate(tqdm(dataset)):    #for each document in the dataset\n",
    "            word_freq = dict(Counter(row.split()))  #frequency of word in row\n",
    "            for word, freq in word_freq.items():  # for each unique word in corpus\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                col_index = vocab.get(word,-1)  #retreving the dimension number of a word, if not exists return -1\n",
    "                if col_index !=-1:  #if word exists in vocab\n",
    "                    rows.append(idx)             #storing row no and\n",
    "                    columns.append(col_index)   #dimension no for getting sparse matrix\n",
    "                    idfvalue=list_idf[col_index]  #getting idf value from list_idf\n",
    "                    tf=float(freq/len(row.split())) #term frequency of word\n",
    "                    values.append(tf*idfvalue) #calculating tfidf value\n",
    "        return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n",
    "        \n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "vocab,list_idf= fit(corpus)\n",
    "print(list(vocab.keys()))\n",
    "print(list_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 799.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n",
      "  (1, 1)\t0.6876235979836937\n",
      "  (1, 3)\t0.2810886740337529\n",
      "  (1, 5)\t0.5386476208856762\n",
      "  (1, 6)\t0.2810886740337529\n",
      "  (1, 8)\t0.2810886740337529\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (3, 1)\t0.4697913855799205\n",
      "  (3, 2)\t0.580285823684436\n",
      "  (3, 3)\t0.3840852409148149\n",
      "  (3, 6)\t0.3840852409148149\n",
      "  (3, 8)\t0.3840852409148149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output=transform(corpus,vocab)\n",
    "tfidf=normalize(output)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n"
     ]
    }
   ],
   "source": [
    "print(tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement max features functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "# Here corpus is of list type\n",
    "\n",
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate idf value\n",
    "\n",
    "def idf(word,dataset):\n",
    "    N=len(dataset)   #length of corpus\n",
    "    No_of_docs=0\n",
    "    for i in range(N):\n",
    "        if word in dataset[i].split():  #splitting document by document in corpus\n",
    "            No_of_docs+=1\n",
    "    if No_of_docs>0:\n",
    "        idfvalue=1+math.log((1+N)/(1+No_of_docs))\n",
    "        return idfvalue\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset):    \n",
    "    unique_words = set()\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for row in dataset:   # for each document in the dataset\n",
    "            for word in row.split(\" \"):  # for each word in the review.\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words=list(unique_words) #unique words in corpus\n",
    "        list2=[]\n",
    "        for word in unique_words:\n",
    "            list2.append(idf(word,corpus))  #calculating idf values for each unique word\n",
    "        list_1=np.array(unique_words)\n",
    "        list_2=np.array(list2)\n",
    "        index=np.argsort(list_2*-1)[:50]  #sorting idf_values array by index and index_list  contains indices of top 50 idfvalues\n",
    "        list_words=np.array(list_1)[index]  #getting top 50 idf words using index and storing in list_words array\n",
    "        list_idf=np.array(list_2)[index]  #getting top 50 idf values using index and storing in list_idf array\n",
    "        index2=np.argsort(list_words)            #sorting list_words (alphabetic order) by index\n",
    "        final_list=np.array(list_words)[index2]  #and storing in final_list array\n",
    "        final_idf=np.array(list_idf)[index2]     #and idfs of sorted list_words\n",
    "        vocab={j:i for i,j in enumerate(final_list)}\n",
    "        return vocab,final_idf\n",
    "    else:\n",
    "        print(\"you need to pass list of sentance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset,vocab):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for idx, row in enumerate(tqdm(dataset)):  #for each document in the dataset\n",
    "            word_freq = dict(Counter(row.split()))  #frequency of word in row\n",
    "            for word, freq in word_freq.items():   #for each unique word in corpus\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                col_index = vocab.get(word,-1)  #retreving the dimension number of a word, if not exists return -1\n",
    "                if col_index !=-1:  #if word exists in vocab\n",
    "                    rows.append(idx)           #storing row no and\n",
    "                    columns.append(col_index)   #dimension no for getting sparse matrix\n",
    "                    idfvalue=list_idf[col_index]  #getting idf value from list_idf\n",
    "                    tf=float(freq/len(row.split()))   #term frequency of word\n",
    "                    values.append(idfvalue*tf)   #calculating tfidf value\n",
    "        return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admitted', 'aspects', 'atrocity', 'ben', 'bible', 'brevity', 'brings', 'brother', 'characterisation', 'commentary', 'cutie', 'dealt', 'delete', 'experiences', 'frances', 'hes', 'humans', 'indication', 'interacting', 'jack', 'kill', 'kristoffersen', 'laselva', 'layers', 'letting', 'logic', 'london', 'maker', 'murdering', 'oriented', 'patriotism', 'person', 'planned', 'politically', 'ponyo', 'preservation', 'relaxing', 'renowned', 'revenge', 'ryan', 'shakespears', 'struggle', 'student', 'suspension', 'teacher', 'unneeded', 'unrealistic', 'vehicles', 'versus', 'water']\n",
      "[6.922918 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918\n",
      " 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918\n",
      " 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918\n",
      " 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918\n",
      " 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918\n",
      " 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918 6.922918\n",
      " 6.922918 6.922918]\n"
     ]
    }
   ],
   "source": [
    "vocab,list_idf=fit(corpus)\n",
    "print(list(vocab.keys()))\n",
    "print(list_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 746/746 [00:00<00:00, 39367.78it/s]\n"
     ]
    }
   ],
   "source": [
    "output=transform(corpus,vocab)\n",
    "tfidf=normalize(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(746, 50)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11)\t0.3535533905932738\n",
      "  (0, 20)\t0.3535533905932738\n",
      "  (0, 25)\t0.3535533905932738\n",
      "  (0, 30)\t0.3535533905932738\n",
      "  (0, 49)\t0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "print(tfidf[135])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.35355339\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35355339 0.         0.         0.\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.35355339 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf[135].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
